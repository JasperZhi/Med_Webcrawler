import os
import time
import random
import requests
import logging
from urllib.parse import unquote
import re
from DrissionPage import ChromiumOptions, ChromiumPage

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# è®¾ç½®å‚æ•°
START_PAGE = 16
END_PAGE = 17
BASE_URL = "https://guidelines.ebmportal.com/?fv%5Bfield_collection_field_4%5D%5B2942%5D=2942&fv%5Bfield_collection_field_4%5D%5B2796%5D=2796&l=50&page={}"
DOWNLOAD_DIR = "/Users/xjz/Desktop/Crawled_1"

# åˆ›å»ºä¸‹è½½ç›®å½•
os.makedirs(DOWNLOAD_DIR, exist_ok=True)

def setup_browser():
    """è®¾ç½®æµè§ˆå™¨é…ç½®"""
    options = ChromiumOptions()
    options.auto_port = True
    options.set_paths(browser_path='/Applications/Google Chrome.app/Contents/MacOS/Google Chrome')
    
    # è®¾ç½®headlessæ¨¡å¼
    options.set_argument('--headless=new')
    options.set_argument('--disable-gpu')
    options.set_argument('--no-sandbox')
    options.set_argument('--disable-dev-shm-usage')
    
    return options

def random_delay():
    """æ¨¡æ‹Ÿäººç±»æ“ä½œå»¶è¿Ÿ"""
    time.sleep(random.uniform(1, 3))

def download_pdf(url, filename):
    """ä¸‹è½½PDFæ–‡ä»¶"""
    try:
        # è§£ç URLç¼–ç çš„æ–‡ä»¶å
        filename = unquote(filename)
        # ç§»é™¤æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*]', '', filename)
        # ç¡®ä¿æ–‡ä»¶åä»¥.pdfç»“å°¾
        if not filename.lower().endswith('.pdf'):
            filename += '.pdf'
        
        filepath = os.path.join(DOWNLOAD_DIR, filename)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”æœ‰æ•ˆ
        if os.path.exists(filepath):
            file_size = os.path.getsize(filepath)
            if file_size > 10240:  # å¤§äº10KBçš„æ–‡ä»¶è®¤ä¸ºæ˜¯æœ‰æ•ˆçš„
                logging.info(f"æ–‡ä»¶å·²å­˜åœ¨ä¸”æœ‰æ•ˆ: {filename} ({file_size} bytes)")
                return True
            else:
                logging.warning(f"å·²å­˜åœ¨çš„æ–‡ä»¶è¿‡å°ï¼Œå°†é‡æ–°ä¸‹è½½: {filename} ({file_size} bytes)")
                os.remove(filepath)

        # ä½¿ç”¨requestsä¸‹è½½æ–‡ä»¶
        response = requests.get(url, stream=True, verify=False, timeout=30)
        response.raise_for_status()

        # æ£€æŸ¥å†…å®¹ç±»å‹
        content_type = response.headers.get('content-type', '')
        if 'pdf' not in content_type.lower():
            logging.warning(f"éPDFæ–‡ä»¶: {url} (Content-Type: {content_type})")
            return False

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        content_length = int(response.headers.get('content-length', 0))
        if content_length < 10240:  # å°äº10KBçš„æ–‡ä»¶å¯èƒ½æ˜¯æ— æ•ˆçš„
            logging.warning(f"æ–‡ä»¶å¤ªå°: {url} ({content_length} bytes)")
            return False

        # ä¿å­˜æ–‡ä»¶
        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)

        logging.info(f"æˆåŠŸä¸‹è½½: {filename}")
        return True

    except Exception as e:
        logging.error(f"ä¸‹è½½å¤±è´¥ {url}: {str(e)}")
        return False

def process_page(page, page_url):
    """å¤„ç†å•ä¸ªé¡µé¢"""
    try:
        page.get(page_url)
        random_delay()

        # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        links = page.s_eles('a')
        pdf_links = []

        for link in links:
            try:
                href = link.attr('href')
                text = link.text.strip()
                if not href or not text:
                    continue

                # æ£€æŸ¥æ˜¯å¦æ˜¯PDFé“¾æ¥æˆ–"view publication"é“¾æ¥
                if href.lower().endswith('.pdf') or 'pdf' in href.lower() or 'view publication' in text.lower():
                    pdf_links.append(href)
            except Exception as e:
                logging.error(f"å¤„ç†é“¾æ¥å¤±è´¥: {str(e)}")
                continue

        if not pdf_links:
            logging.warning("âš ï¸ æœªæ‰¾åˆ°PDFé“¾æ¥")
            return 0

        logging.info(f"æ‰¾åˆ° {len(pdf_links)} ä¸ªPDFé“¾æ¥")
        success_count = 0
        
        for pdf_url in pdf_links:
            # ä» URL æå–çœŸæ­£çš„æ–‡ä»¶å
            filename = unquote(pdf_url.split('/')[-1])
            if download_pdf(pdf_url, filename):
                success_count += 1
            random_delay()

        return success_count

    except Exception as e:
        logging.error(f"å¤„ç†é¡µé¢å¤±è´¥ {page_url}: {str(e)}")
        return 0

def main():
    total_pages = END_PAGE - START_PAGE + 1
    total_downloads = 0
    
    # åˆ›å»ºä¸»æµè§ˆå™¨å®ä¾‹
    page = ChromiumPage(setup_browser())
    try:
        for page_num in range(START_PAGE, END_PAGE + 1):
            logging.info(f"\nğŸ“– å¼€å§‹å¤„ç†ç¬¬ {page_num} é¡µ...")
            page_url = BASE_URL.format(page_num)
            logging.info(f"\nğŸ” å¤„ç†ä¸»é¡µé¢: {page_url}")
            
            downloads = process_page(page, page_url)
            total_downloads += downloads
            logging.info(f"æœ¬é¡µæˆåŠŸä¸‹è½½ {downloads} ä¸ªPDF")
            random_delay()

        logging.info(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼å…±å¤„ç† {total_pages} é¡µï¼ŒæˆåŠŸä¸‹è½½ {total_downloads} ä¸ªPDF")
        logging.info(f"PDFå·²ä¿å­˜åˆ° {DOWNLOAD_DIR}")
    finally:
        page.quit()

if __name__ == "__main__":
    main() 
